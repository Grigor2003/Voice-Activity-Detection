{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87cbdd0b96559d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:32:16.311989Z",
     "start_time": "2025-04-13T10:32:13.789418Z"
    }
   },
   "outputs": [],
   "source": [
    "import wave\n",
    "from time import time\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import webrtcvad\n",
    "import contextlib\n",
    "import soundfile as sf\n",
    "import scipy.io.wavfile as wav\n",
    "import resampy\n",
    "\n",
    "from other.utils import get_files_by_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6fdbf9523ce23d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:32:16.329518Z",
     "start_time": "2025-04-13T10:32:16.319503Z"
    }
   },
   "outputs": [],
   "source": [
    "class WebrtcVadLabelMaker:\n",
    "    @staticmethod\n",
    "    def resample_pcm_wav(file_path, target_sr, output_path=None):\n",
    "        \"\"\"\n",
    "        Reads a PCM-formatted WAV file, resamples if the sample rate differs from target_sr,\n",
    "        and saves the output as a PCM WAV file.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path: str, path to the input WAV file.\n",
    "        - target_sr: int, target sample rate (Hz).\n",
    "        - output_path: str (optional), path to save the resampled WAV file.\n",
    "                    If None, the output is saved as \"<original_filename>_resampled.wav\" in the same directory.\n",
    "        \"\"\"\n",
    "        # Read original wav file\n",
    "        orig_sr, data = wav.read(file_path)\n",
    "\n",
    "        # Check if resampling is needed\n",
    "        if orig_sr == target_sr:\n",
    "            if output_path is None:\n",
    "                output_path = f\"{os.path.splitext(file_path)[0]}_copy.wav\"\n",
    "            wav.write(output_path, orig_sr, data)\n",
    "            return\n",
    "\n",
    "        # Convert data to float32 for processing. PCM int16 data ranges -32768 to 32767.\n",
    "        data_float = data.astype(np.float32)\n",
    "        if data.dtype == np.int16:\n",
    "            data_float /= 32768.0  # Normalize to roughly [-1, 1)\n",
    "\n",
    "        # Resample the data.\n",
    "        # Handle mono and multi-channel audio:\n",
    "        if data_float.ndim == 1:\n",
    "            data_resampled = resampy.resample(data_float, orig_sr, target_sr)\n",
    "        else:\n",
    "            # For multi-channel, resample each channel separately.\n",
    "            channels = []\n",
    "            for ch in range(data_float.shape[1]):\n",
    "                ch_resampled = resampy.resample(data_float[:, ch], orig_sr, target_sr)\n",
    "                channels.append(ch_resampled)\n",
    "            # Stack channels back into a 2D array (samples x channels)\n",
    "            data_resampled = np.stack(channels, axis=-1)\n",
    "\n",
    "        # Convert resampled data back to int16.\n",
    "        # Scale the float data back to the int16 range and clip to avoid overflow.\n",
    "        data_resampled = np.clip(data_resampled * 32768, -32768, 32767).astype(np.int16)\n",
    "\n",
    "        # Determine the output path if not provided.\n",
    "        if output_path is None:\n",
    "            base, ext = os.path.splitext(file_path)\n",
    "            output_path = f\"{base}_resampled.wav\"\n",
    "\n",
    "        # Save the resampled file.\n",
    "        wav.write(output_path, target_sr, data_resampled)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_wave(path, target_sr=None):\n",
    "        ext = os.path.splitext(path)[1]\n",
    "        if ext == '.wav':\n",
    "            with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "                comp_type = wf.getcomptype()\n",
    "                assert comp_type == 'NONE'\n",
    "            WebrtcVadLabelMaker.resample_pcm_wav(path, target_sr, path)\n",
    "            with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "                num_channels = wf.getnchannels()\n",
    "                assert num_channels == 1\n",
    "                sample_width = wf.getsampwidth()\n",
    "                assert sample_width == 2\n",
    "                sample_rate = wf.getframerate()\n",
    "                assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "                pcm_data = wf.readframes(wf.getnframes())\n",
    "                return pcm_data, sample_rate\n",
    "        elif ext == '.flac':\n",
    "            with sf.SoundFile(path, \"r\") as flac_file:\n",
    "                pcm_data = flac_file.read(dtype=\"int16\").tobytes()\n",
    "                sample_rate = flac_file.samplerate\n",
    "                assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "                num_channels = flac_file.channels\n",
    "                assert num_channels == 1\n",
    "                return pcm_data, sample_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def find_ones_regions(arr, threshold=0):\n",
    "        diff = np.diff(arr)\n",
    "        starts = np.where(diff == 1)[0] + 1  # +1 because diff shifts left\n",
    "        ends = np.where(diff == -1)[0]\n",
    "\n",
    "        # Handle edge cases\n",
    "        if arr[0] == 1:\n",
    "            starts = np.insert(starts, 0, 0)\n",
    "        if arr[-1] == 1:\n",
    "            ends = np.append(ends, len(arr) - 1)\n",
    "\n",
    "        # Ensure starts and ends are the same length\n",
    "        if len(starts) != len(ends):\n",
    "            raise ValueError(\"Mismatch between starts and ends\")\n",
    "\n",
    "        # Flatten the starts and ends into a single list\n",
    "        result = []\n",
    "        for s, e in zip(starts, ends):\n",
    "            if e - s < threshold:\n",
    "                continue\n",
    "            result.extend([s, e])\n",
    "\n",
    "        if len(result) > 2:\n",
    "            result = [result[0], result[-1]]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __init__(self, mode=2, vad_window_ms=30, min_region_ms=30, vad_overlap_ratio=0, target_sample_rate=16000, decider_function=None):\n",
    "        self.vad_window_ms = vad_window_ms\n",
    "        self.vad_overlap_ratio = vad_overlap_ratio\n",
    "        self.vad = webrtcvad.Vad(mode)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.decider_function = decider_function\n",
    "        self.min_region_ms = min_region_ms\n",
    "\n",
    "    def __call__(self, file_path):\n",
    "        wave, rate = WebrtcVadLabelMaker.read_wave(file_path, target_sr=self.target_sample_rate)\n",
    "        if rate != self.target_sample_rate:\n",
    "            print(f\"{file_path} has a rate of {rate} instead of {self.target_sample_rate}\")\n",
    "            return\n",
    "        window = int(self.vad_window_ms * rate / 1000)\n",
    "        step = int((1 - self.vad_overlap_ratio) * window)\n",
    "\n",
    "        samples_count = len(wave) // 2\n",
    "        samples_pred_sum = np.zeros(len(wave), dtype=np.float32)\n",
    "        samples_pred_count = np.zeros(len(wave), dtype=np.float32)\n",
    "\n",
    "        n_frames = int((samples_count - window) / step)\n",
    "        for i in range(n_frames):\n",
    "            s = i * step\n",
    "            e = s + window\n",
    "            is_speech = self.vad.is_speech(wave[2 * s:2 * e], rate)\n",
    "            samples_pred_sum[s:e] += is_speech\n",
    "            samples_pred_count[s:e] += 1\n",
    "\n",
    "        samples_pred = (samples_pred_sum / (samples_pred_count + 1e-8)) >= 0.5\n",
    "        ones_regions = self.find_ones_regions(samples_pred.astype(np.int32), threshold=self.min_region_ms * rate / 1000)\n",
    "\n",
    "        return ones_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SileroVadLabelMaker:\n",
    "    def __init__(self, sample_rate=8000):\n",
    "        self.model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "        (self.get_speech_timestamps, _, self.read_audio, _, _) = utils\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def __call__(self, file_path):\n",
    "        wav = self.read_audio(file_path, sampling_rate=self.sample_rate)\n",
    "        speech_timestamps = self.get_speech_timestamps(\n",
    "        wav,\n",
    "        self.model,\n",
    "        return_seconds=False,\n",
    "        sampling_rate=self.sample_rate,\n",
    "        # min_silence_duration_ms=50,\n",
    "        # min_speech_duration_ms=150,\n",
    "        )\n",
    "        ones_regions = []\n",
    "        for stamps in speech_timestamps:\n",
    "            ones_regions.append(stamps['start'])\n",
    "            ones_regions.append(stamps['end'])\n",
    "        # if len(ones_regions) > 0:\n",
    "        #     ones_regions = [ones_regions[0], ones_regions[1]]\n",
    "        return ones_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4fd30a1a8f0f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:32:16.608224Z",
     "start_time": "2025-04-13T10:32:16.605720Z"
    }
   },
   "outputs": [],
   "source": [
    "target_sample_rate = 8000\n",
    "vad_window_ms = [10, 20, 30][2]\n",
    "vad_overlap_ratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f0353eee9da988",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:33:16.181556Z",
     "start_time": "2025-04-13T10:33:15.543049Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\narek/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17665 files like: down\\627c0bec_nohash_2.wav\n",
      "buffer\\8000_silerovad_labels.csv\n"
     ]
    }
   ],
   "source": [
    "openSLR_data_directory, ext = 'datasets\\google_commands_v2', 'wav'\n",
    "# openSLR_data_directory, ext = \"../data/MSDWild/raw_wav\", 'wav'\n",
    "where_to_save = 'buffer'\n",
    "\n",
    "# vad = WebrtcVadLabelMaker(\n",
    "#     mode=2,\n",
    "#     vad_window_ms=vad_window_ms,\n",
    "#     vad_overlap_ratio=vad_overlap_ratio, \n",
    "#     target_sample_rate=target_sample_rate,\n",
    "#     min_region_ms=60)\n",
    "vad = SileroVadLabelMaker()\n",
    "\n",
    "audio_files_paths = get_files_by_extension(openSLR_data_directory, ext=ext, rel=True)\n",
    "\n",
    "# labels_path = f'{vad.target_sample_rate}_{vad.vad_window_ms}_{int(vad.vad_overlap_ratio * 100)}_webrtc_labels.csv'\n",
    "labels_path = f'{vad.sample_rate}_silerovad_labels.csv'\n",
    "labels_path = os.path.join(where_to_save, labels_path)\n",
    "os.makedirs(where_to_save, exist_ok=True)\n",
    "data_samples = len(audio_files_paths)\n",
    "print(data_samples, \"files like:\", np.random.choice(audio_files_paths))\n",
    "print(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec8fc383f9551dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:59:36.181805Z",
     "start_time": "2025-04-13T10:36:16.050315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "webrtcvad: 34.8ms | write: 27.8ms:  50%|█████     | 8857/17665 [04:15<04:14, 34.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m s_vad = time()\n\u001b[32m     10\u001b[39m filepath = os.path.join(openSLR_data_directory, audio_path)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m one_stamps = \u001b[43mvad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m one_stamps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSileroVadLabelMaker.__call__\u001b[39m\u001b[34m(self, file_path)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path):\n\u001b[32m      8\u001b[39m     wav = \u001b[38;5;28mself\u001b[39m.read_audio(file_path, sampling_rate=\u001b[38;5;28mself\u001b[39m.sample_rate)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     speech_timestamps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_speech_timestamps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_silence_duration_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_speech_duration_ms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     ones_regions = []\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stamps \u001b[38;5;129;01min\u001b[39;00m speech_timestamps:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\narek\\EpicDocuments\\PythonProjects\\vad_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache\\torch\\hub\\snakers4_silero-vad_master\\src\\silero_vad\\utils_vad.py:288\u001b[39m, in \u001b[36mget_speech_timestamps\u001b[39m\u001b[34m(audio, model, threshold, sampling_rate, min_speech_duration_ms, max_speech_duration_s, min_silence_duration_ms, speech_pad_ms, return_seconds, visualize_probs, progress_tracking_callback, window_size_samples)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) < window_size_samples:\n\u001b[32m    287\u001b[39m     chunk = torch.nn.functional.pad(chunk, (\u001b[32m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(window_size_samples - \u001b[38;5;28mlen\u001b[39m(chunk))))\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m speech_prob = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m.item()\n\u001b[32m    289\u001b[39m speech_probs.append(speech_prob)\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# caculate progress and seng it to callback function\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\narek\\EpicDocuments\\PythonProjects\\vad_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\narek\\EpicDocuments\\PythonProjects\\vad_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if data_samples > 0:\n",
    "    with open(labels_path, 'w') as file:\n",
    "        file.write(\"filename,labels\" + '\\n')\n",
    "\n",
    "        t = tqdm(audio_files_paths, total=data_samples)\n",
    "        webrtcvad_t, write_t = 0, 0\n",
    "        ma = 0.8\n",
    "        for i, audio_path in enumerate(t):\n",
    "            s_vad = time()\n",
    "            filepath = os.path.join(openSLR_data_directory, audio_path)\n",
    "            one_stamps = vad(filepath)\n",
    "            if one_stamps is None:\n",
    "                continue\n",
    "            e_vad = time()\n",
    "            path_parts = audio_path.split(os.sep)\n",
    "            filename = path_parts[-1]\n",
    "\n",
    "\n",
    "            file.write(audio_path + ',' + '-'.join(map(str, one_stamps)) + '\\n')\n",
    "            e_write = time()\n",
    "\n",
    "            webrtcvad_t = ma * webrtcvad_t + (1 - ma) * (e_vad - s_vad)\n",
    "            write_t = ma * webrtcvad_t + (1 - ma) * (e_write - e_vad)\n",
    "            if i % 100 == 0:\n",
    "                t.set_description_str(f\"webrtcvad: {webrtcvad_t * 1000:.1f}ms | write: {write_t * 1000:.1f}ms\")\n",
    "\n",
    "else:\n",
    "    print(len(audio_files_paths), \"audio files not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255df167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:00:30.372985Z",
     "start_time": "2025-04-13T11:00:30.024666Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(labels_path)\n",
    "\n",
    "problematics = df[df.isnull().any(axis=1)].filename.values.tolist()\n",
    "problematics = set(os.path.basename(problematic).split('_')[0] for problematic in problematics)\n",
    "len(problematics)\n",
    "\n",
    "mask = df['filename'].apply(lambda x: any(p in x for p in problematics))\n",
    "\n",
    "df = df[~mask]\n",
    "df.to_csv(labels_path.replace('.csv', '_filtered.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vad_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
